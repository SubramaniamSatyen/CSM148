{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-m80KgQ1-Z3"
      },
      "source": [
        "## 24W-COM SCI-M148 Project 2 - Binary Classification Comparative Methods\n",
        "\n",
        "Name: Satyen Subramaniam\n",
        "\n",
        "UID: 005737472\n",
        "\n",
        "### **Submission Guidelines**\n",
        "1. Please fill in your name and UID above.\n",
        "\n",
        "2. Please submit a **PDF printout** of your Jupyter Notebook to **Gradescope**. If you have any trouble accessing Gradescope, please let a TA know ASAP.  \n",
        "\n",
        "3. As the PDF can get long, please tag the respective sections to ensure the readers know where to look.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSR9laCN1-Z5"
      },
      "source": [
        "For this project we're going to attempt a binary classification of a dataset using multiple methods and compare results.\n",
        "\n",
        "Our goals for this project will be to introduce you to several of the most common classification techniques, how to perform them and tweek parameters to optimize outcomes, how to produce and interpret results, and compare performance. You will be asked to analyze your findings and provide explanations for observed performance.\n",
        "\n",
        "Specifically you will be asked to classify whether a <b>patient is suffering from heart disease</b> based on a host of potential medical factors.\n",
        "\n",
        "<b><u>DEFINITIONS</b></u>\n",
        "\n",
        "\n",
        "<b> Binary Classification:</b>\n",
        "In this case a complex dataset has an added 'target' label with one of two options. Your learning algorithm will try to assign one of these labels to the data.\n",
        "\n",
        "<b> Supervised Learning:</b>\n",
        "This data is fully supervised, which means it's been fully labeled and we can trust the veracity of the labeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zacxg4dy1-Z7"
      },
      "source": [
        "## Background: The Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKL6UhzY1-Z8"
      },
      "source": [
        "For this exercise we will be using a subset of the UCI Heart Disease dataset, leveraging the fourteen most commonly used attributes. All identifying information about the patient has been scrubbed.\n",
        "\n",
        "The dataset includes 14 columns. The information provided by each column is as follows:\n",
        "<ul>\n",
        "    <li><b>age:</b> Age in years</li>\n",
        "    <li><b>sex:</b> (1 = male; 0 = female)</li>\n",
        "    <li><b>cp:</b> Chest pain type (0 = asymptomatic; 1 = atypical angina; 2 = non-anginal pain; 3 = typical angina)</li>\n",
        "    <li><b>trestbps:</b> Resting blood pressure (in mm Hg on admission to the hospital)</li>\n",
        "    <li><b>cholserum:</b> Cholestoral in mg/dl</li>\n",
        "    <li><b>fbs</b> Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)</li>\n",
        "    <li><b>restecg:</b> Resting electrocardiographic results (0= showing probable or definite left ventricular hypertrophy by Estes' criteria; 1 = normal; 2 = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV))</li>\n",
        "    <li><b>thalach:</b> Maximum heart rate achieved</li>\n",
        "    <li><b>exang:</b> Exercise induced angina (1 = yes; 0 = no)</li>\n",
        "    <li><b>oldpeakST:</b> Depression induced by exercise relative to rest</li>\n",
        "    <li><b>slope:</b> The slope of the peak exercise ST segment (0 = downsloping; 1 = flat; 2 = upsloping)</li>\n",
        "    <li><b>ca:</b> Number of major vessels (0-3) colored by flourosopy</li>\n",
        "    <li><b>thal:</b> 1 = normal; 2 = fixed defect; 7 = reversable defect</li>\n",
        "    <li><b><u>Sick:</u></b> Indicates the presence of Heart disease (True = Disease; False = No disease)</li>\n",
        "</ul>\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-OS5fRR1-Z9"
      },
      "source": [
        "## Loading Essentials and Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "89pf3hlo1-Z-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\satye\\AppData\\Local\\Temp\\ipykernel_24092\\2374470638.py:4: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
          ]
        }
      ],
      "source": [
        "#Here are a set of libraries we imported to complete this assignment.\n",
        "#Feel free to use these or equivalent libraries for your implementation\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
        "import os\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn.metrics.cluster as smc\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "from matplotlib import pyplot\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLA8MesY1-aD"
      },
      "source": [
        "## Part 1. Load the Data and Analyze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cPvNFAb1-aD"
      },
      "source": [
        "Let's first load our dataset so we'll be able to work with it. (correct the relative path if your notebook is in a different directory than the csv file.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yeQ4T7nD1-aE"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('heartdisease.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR8RcYxs1-aF"
      },
      "source": [
        "### Now that our data is loaded, let's take a closer look at the dataset we're working with. Use the head method,  the describe method, and the info method to display some of the rows so we can visualize the types of data fields we'll be working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aPdUGLr21-aF"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>sick</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
              "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
              "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
              "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
              "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
              "\n",
              "   ca  thal   sick  \n",
              "0   0     1  False  \n",
              "1   0     2  False  \n",
              "2   0     2  False  \n",
              "3   0     2  False  \n",
              "4   0     2  False  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "muZ1U7Xw1-aG"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "      <td>303.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>54.366337</td>\n",
              "      <td>0.683168</td>\n",
              "      <td>0.966997</td>\n",
              "      <td>131.623762</td>\n",
              "      <td>246.264026</td>\n",
              "      <td>0.148515</td>\n",
              "      <td>0.528053</td>\n",
              "      <td>149.646865</td>\n",
              "      <td>0.326733</td>\n",
              "      <td>1.039604</td>\n",
              "      <td>1.399340</td>\n",
              "      <td>0.729373</td>\n",
              "      <td>2.313531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.082101</td>\n",
              "      <td>0.466011</td>\n",
              "      <td>1.032052</td>\n",
              "      <td>17.538143</td>\n",
              "      <td>51.830751</td>\n",
              "      <td>0.356198</td>\n",
              "      <td>0.525860</td>\n",
              "      <td>22.905161</td>\n",
              "      <td>0.469794</td>\n",
              "      <td>1.161075</td>\n",
              "      <td>0.616226</td>\n",
              "      <td>1.022606</td>\n",
              "      <td>0.612277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>47.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>211.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>133.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>55.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>240.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>153.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>274.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>166.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              age         sex          cp    trestbps        chol         fbs  \\\n",
              "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
              "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
              "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
              "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
              "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
              "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
              "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
              "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
              "\n",
              "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
              "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
              "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
              "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
              "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
              "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
              "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
              "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
              "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
              "\n",
              "             thal  \n",
              "count  303.000000  \n",
              "mean     2.313531  \n",
              "std      0.612277  \n",
              "min      0.000000  \n",
              "25%      2.000000  \n",
              "50%      2.000000  \n",
              "75%      3.000000  \n",
              "max      3.000000  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TS41dFMJYskm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 303 entries, 0 to 302\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       303 non-null    int64  \n",
            " 1   sex       303 non-null    int64  \n",
            " 2   cp        303 non-null    int64  \n",
            " 3   trestbps  303 non-null    int64  \n",
            " 4   chol      303 non-null    int64  \n",
            " 5   fbs       303 non-null    int64  \n",
            " 6   restecg   303 non-null    int64  \n",
            " 7   thalach   303 non-null    int64  \n",
            " 8   exang     303 non-null    int64  \n",
            " 9   oldpeak   303 non-null    float64\n",
            " 10  slope     303 non-null    int64  \n",
            " 11  ca        303 non-null    int64  \n",
            " 12  thal      303 non-null    int64  \n",
            " 13  sick      303 non-null    bool   \n",
            "dtypes: bool(1), float64(1), int64(12)\n",
            "memory usage: 31.2 KB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hl4xIdL1-aM"
      },
      "source": [
        "### Before we begin our analysis we need to fix the field(s) that will be problematic. Specifically convert our boolean sick variable into a binary numeric target variable (values of either '0' or '1'), and then drop the original sick datafield from the dataframe. (hint: try label encoder or .astype()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDmfWcDO1-aM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDbsCOog1-aN"
      },
      "source": [
        "### Now that we have a feel for the data-types for each of the variables, plot histograms of each field and attempt to ascertain how each variable performs (is it a binary, or limited selection, or does it follow a gradient?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYlbbySv1-aO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZwpu5xZ1-aP"
      },
      "source": [
        "### We also want to make sure we are dealing with a balanced dataset. In this case, we want to confirm whether or not we have an equitable number of  sick and healthy individuals to ensure that our classifier will have a sufficiently balanced dataset to adequately classify the two. Plot a histogram specifically of the sick target, and conduct a count of the number of sick and healthy individuals and report on the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo0tH8sS1-aP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v1_1HFC1-aS"
      },
      "source": [
        "### Now that we have our dataframe prepared let's start analyzing our data. For this next question let's look at the correlations of our variables to our target value. First, map out the correlations between the values, and then discuss the relationships you observe. Do some research on the variables to understand why they may relate to the observed corellations. Intuitively, why do you think some variables correlate more highly than others (hint: one possible approach you can use the sns heatmap function to map the corr() method)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXAfCjem1-aS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osMk-hMm1-aT"
      },
      "source": [
        "[Discuss correlations here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc_52Cut1-aT"
      },
      "source": [
        "## Part 2. Prepare the 'Raw' Data and run a KNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ6wr3Xc1-aU"
      },
      "source": [
        "Before running our various learning methods, we need to do some additional prep to finalize our data. Specifically you'll have to cut the classification target from the data that will be used to classify, and then you'll have to divide the dataset into training and testing cohorts.\n",
        "\n",
        "Specifically, we're going to ask you to prepare 2 batches of data: 1. Will simply be the raw numeric data that hasn't gone through any additional pre-processing. The other, will be data that you pipeline using your own selected methods. We will then feed both of these datasets into a classifier to showcase just how important this step can be!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piz-mp0o1-aV"
      },
      "source": [
        "### Save the label column as a separate array and then drop it from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8svzGB-A1-aV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG7ri8V11-aW"
      },
      "source": [
        "### First Create your 'Raw' unprocessed training data by dividing your dataframe into training and testing cohorts, with your training cohort consisting of 70% of your total dataframe (hint: use the train_test_split() method) Output the resulting shapes of your training and testing samples to confirm that your split was successful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmjpma2F1-aX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N5-dXElZKE9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7TWR8en1-aa"
      },
      "source": [
        "### We'll explore how not processing your data can impact model performance by using the K-Nearest Neighbor classifier. One thing to note was because KNN's rely on Euclidean distance, they are highly sensitive to the relative magnitude of different features. Let's see that in action! Implement a K-Nearest Neighbor algorithm on our raw data and report the results. For this initial implementation simply use the default settings. Refer to the [KNN Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) for details on implementation. Report on the accuracy of the resulting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D60t7Qe31-ab"
      },
      "outputs": [],
      "source": [
        "# k-Nearest Neighbors algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeGfA2Cf1-ab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofBgJMdH1-ac"
      },
      "source": [
        "### Now implement a pipeline of your choice. You can opt to handle categoricals however you wish, however please scale your numeric features using standard scaler. Use the fit_transform() to fit this pipeline to your training data. and then transform() to apply that pipeline to your test data\n",
        "\n",
        "Hint:\n",
        "1. Create separate pipelines for numeric and categorical features with Pipeline() and then combining them with ColumnTransformer()\n",
        "2. First, fit the full pipeline with the training data. Then, apply it to the test data as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xySOY-yS1-ad"
      },
      "source": [
        "### Pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgX7OEo61-af"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mde5kr1Z9fS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHQZzYiTJ3US"
      },
      "outputs": [],
      "source": [
        "# Pipeline my test data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMbUOszrJOM0"
      },
      "source": [
        "### Now retrain your model and compare the accuracy metrics (Accuracy, Precision, Recall, F1 Score) with the raw and pipelined data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4JqymGE1-an"
      },
      "outputs": [],
      "source": [
        "\n",
        "# k-Nearest Neighbors algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCRmyi1Y1-an"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjDfPNK01-ao"
      },
      "source": [
        "[Discuss Results here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3Kx4nH91-ap"
      },
      "source": [
        "### Parameter Optimization.  The KNN Algorithm includes an n_neighbors attribute that specifies how many neighbors to use when developing the cluster. (The default value is 5, which is what your previous model used.) Lets now try n values of: 1, 2, 3, 5, 7, 9, 10, 20, and 50. Run your model for each value and report the accuracy for each. (HINT leverage python's ability to loop to run through the array and generate results without needing to manually code each iteration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aQa6hMT1-aq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq4htLuI1-aq"
      },
      "source": [
        "## Part 3. Additional Learning Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2pXCSFS1-ar"
      },
      "source": [
        "So we have a model that seems to work well. But let's see if we can do better! To do so we'll employ multiple learning methods and compare result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qs89tu81-ar"
      },
      "source": [
        "### Linear Decision Boundary Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaZ_NkZM1-as"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtWdZgsY1-as"
      },
      "source": [
        "Let's now try another classifier,one that's well known for handling linear models: Logistic Regression. Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycUesx_E1-at"
      },
      "source": [
        "### Implement a Logistical Regression Classifier. Review the [Logistical Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for how to implement the model.\n",
        "\n",
        "### Report metrics for:\n",
        "1.   Accuracy\n",
        "2.   Precision\n",
        "3.   Recall\n",
        "4.   F1 Score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZtnRX5f1-au"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxc0eyNm1-a7"
      },
      "source": [
        "### Discuss what each measure is reporting, why they are different, and why are each of these measures is significant. Explore why we might choose to evaluate the performance of differing models differently based on these factors. Try to give some specific examples of scenarios in which you might value one of these measures over the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu8lKGYL1-a7"
      },
      "source": [
        "[Provide explanation for each measure here]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVc5lMGZ1-av"
      },
      "source": [
        "### Let's tweak a few settings. First let's set your solver to 'sag', your max_iter= 10, and set penalty = 'none' and rerun your model. Let's see how your results change!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82CwH2261-aw"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaLDf1Wn1-ax"
      },
      "source": [
        "### Did you notice that when you ran the previous model you got the following warning: \"ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\". Check the documentation and see if you can implement a fix for this problem, and again report your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGaSV3wo1-ay"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1YFl1Y1-a0"
      },
      "source": [
        "### Explain what you changed, and why do you think that may have altered the outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYiZLct01-a1"
      },
      "source": [
        "[Provide explanation here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M59dRwK81-a2"
      },
      "source": [
        "### Rerun your logistic classifier, but modify the penalty = 'l1', solver='liblinear' and again report the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xsw3IKrs1-a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUxx3eFy1-a3"
      },
      "source": [
        "### Explain what what the two solver approaches are, and why liblinear may have produced an improved outcome (but not always, and it's ok if your results show otherwise!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJZIVyvS1-a4"
      },
      "source": [
        "[Provide explanation here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C95mrPjA1-a4"
      },
      "source": [
        "### SVM (Support Vector Machine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJYSnWKf1-a5"
      },
      "source": [
        "A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples. In two dimentional space this hyperplane is a line dividing a plane in two parts where in each class lay in either side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUAfw9mF1-a5"
      },
      "source": [
        "### Implement a Support Vector Machine classifier on your pipelined data. Review the [SVM Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for how to implement a model. For this implementation you can simply use the default settings, but set probability = True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kT3sHrW1-a6"
      },
      "outputs": [],
      "source": [
        "# SVM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVNvSFJt1-a6"
      },
      "source": [
        "### Report the accuracy, precision, recall, F1 Score, of your model, but in addition, plot a Confusion Matrix of your model's performance\n",
        "\n",
        "recommend using `from sklearn.metrics import ConfusionMatrixDisplay` for this one!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3If-zbZG1-a6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzc0MVWDe7w7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaXoiMYd1-a8"
      },
      "source": [
        "### Plot a Receiver Operating Characteristic curve, or ROC curve, and describe what it is and what the results indicate\n",
        "\n",
        "recommend using the `metrics.roc_curve` `metrics.auc` and `metrics.RocCurveDisplay` for this one!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pCVtTnK1-a8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7NfLZoL1-a9"
      },
      "source": [
        "[Describe what an ROC Curve is and what the results mean here] The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The area under an ROC curve is a measure of the usefulness of a test in general, where a greater area means a more useful test, so the areas under ROC curves are used to compare the usefulness of tests. Here we see a relatively low area under the curve indicating a poorly performing model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx7L-bgW1-a9"
      },
      "source": [
        "### Rerun your SVM, but now modify your model parameter kernel to equal 'linear'. Again report your Accuracy, Precision, Recall, F1 scores, and Confusion matrix and plot the new ROC curve.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6KpYps71-a-"
      },
      "outputs": [],
      "source": [
        "# SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlnzo0uL1-a-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgTiftKK1-a_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW_lqTMn1-bA"
      },
      "source": [
        "### Explain the what the new results you've achieved mean. Read the documentation to understand what you've changed about your model and explain why changing that input parameter might impact the results in the manner you've observed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdji_tZn1-bA"
      },
      "source": [
        "[Provide Answer here:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uoAdwas1-bB"
      },
      "source": [
        "### Both logistic regression and linear SVM are trying to classify data points using a linear decision boundary, then whatâ€™s the difference between their ways to find this boundary?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZvy7RTF1-bC"
      },
      "source": [
        "[Provide Answer here:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwZkVmKRcKZY"
      },
      "source": [
        "# Printing Jupyter notebook to PDF\n",
        "\n",
        "https://piazza.com/class/lr48piooit63hi/post/47 by Steven Bash\n",
        "\n",
        "It may take a few minutes to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUXRTQ6RcHAG"
      },
      "outputs": [],
      "source": [
        "def colab2pdf():\n",
        "  ENABLE=True # @param {type:\"boolean\"}\n",
        "  if ENABLE:\n",
        "    !apt-get install librsvg2-bin\n",
        "    import os, datetime, json, locale, pathlib, urllib, requests, werkzeug, nbformat, google, yaml, warnings\n",
        "    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "    NAME = pathlib.Path(werkzeug.utils.secure_filename(urllib.parse.unquote(requests.get(f\"http://{os.environ['COLAB_JUPYTER_IP']}:{os.environ['KMP_TARGET_PORT']}/api/sessions\").json()[0][\"name\"])))\n",
        "    TEMP = pathlib.Path(\"/content/pdfs\") / f\"{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{NAME.stem}\"; TEMP.mkdir(parents=True, exist_ok=True)\n",
        "    NB = [cell for cell in nbformat.reads(json.dumps(google.colab._message.blocking_request(\"get_ipynb\", timeout_sec=600)[\"ipynb\"]), as_version=4).cells if \"--Colab2PDF\" not in cell.source]\n",
        "    warnings.filterwarnings('ignore', category=nbformat.validator.MissingIDFieldWarning)\n",
        "    with (TEMP / f\"{NAME.stem}.ipynb\").open(\"w\", encoding=\"utf-8\") as nb_copy: nbformat.write(nbformat.v4.new_notebook(cells=NB or [nbformat.v4.new_code_cell(\"#\")]), nb_copy)\n",
        "  if not pathlib.Path(\"/usr/local/bin/quarto\").exists():\n",
        "    !wget -q \"https://quarto.org/download/latest/quarto-linux-amd64.deb\" -P {TEMP} && dpkg -i {TEMP}/quarto-linux-amd64.deb > /dev/null && quarto install tinytex --update-path --quiet\n",
        "    with (TEMP / \"config.yml\").open(\"w\", encoding=\"utf-8\") as file: yaml.dump({'include-in-header': [{\"text\": r\"\\usepackage{fvextra}\\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines,breakanywhere,commandchars=\\\\\\{\\}}\"}],'include-before-body': [{\"text\": r\"\\DefineVerbatimEnvironment{verbatim}{Verbatim}{breaksymbolleft={},showspaces=false,showtabs=false,breaklines}\"}]}, file)\n",
        "    !quarto render {TEMP}/{NAME.stem}.ipynb --metadata-file={TEMP}/config.yml --to pdf -M latex-auto-install -M margin-top=1in -M margin-bottom=1in -M margin-left=1in -M margin-right=1in --quiet\n",
        "    google.colab.files.download(str(TEMP / f\"{NAME.stem}.pdf\"))\n",
        "colab2pdf()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
